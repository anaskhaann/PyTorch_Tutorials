{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Why we need this**"
      ],
      "metadata": {
        "id": "7QaDCGXTUzGJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code which we have written previously have a **Big Flaw**\n",
        "```py\n",
        "for epoch in range(epochs):\n",
        "\n",
        "  # Forward Pass\n",
        "  y_pred = model.forward(x_train_tensor)\n",
        "\n",
        "  # Loss Calculate\n",
        "  loss = loss_function(y_pred.squeeze(), y_train_tensor)\n",
        "\n",
        "  # Make gradeints zero(Doing it before backward pass bcoz it is suggested.)\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # BackWard Loss(backPropogate)\n",
        "  loss.backward()\n",
        "\n",
        "  # Update weight & bias using optimizer\n",
        "  optimizer.step()\n",
        "\n",
        "  # Print the Loss\n",
        "  print(f\"Epoch:{epoch +1}, Loss:{loss}\")\n",
        "  print('='*60)\n",
        "\n",
        "```\n",
        "\n",
        "In the above code we are looping again & again over the entire dataset i.e(x_train_tensor).\n",
        "\n",
        "- This is very inefficient way. Because we are using Batch Gradient Descent.\n",
        "- Convergence is not that great. Because we are updating the parameter one time by looking at the overall data.\n",
        "\n",
        "---\n",
        "**Solution** : Divide data into Batches and perform on that. And iterate over the batch. This is called as **Mini Batch Gradient Descent.**"
      ],
      "metadata": {
        "id": "1fdtIZE_U2jt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Solution one by using nested loops to iterate over data**\n",
        "\n",
        "This will work but there are some problems with this approach.\n",
        "\n",
        "- Sometime data gathering is very difficult, because suppose there are dataset for images in multiple folder based on categories. So this Approach does not handle that.\n",
        "\n",
        "- Another Problem is There is no transformation in this. Sometime for **RGB** images we required to transform some. Suppose convert colour images to **B/W**.\n",
        "\n",
        "- No Shuffling and Sampling\n",
        "\n",
        "- Batch management & Parallelization.\n",
        "\n",
        "So this will work but it is not good way."
      ],
      "metadata": {
        "id": "tPVkAcNqXGqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "**To Solve this problems we have DataSet & DataLoader Classess**"
      ],
      "metadata": {
        "id": "bC3UhT4PZcIq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How Dataset & DataLoader Works**"
      ],
      "metadata": {
        "id": "qHbQ4LIJZomm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset and DataLoader are core abstractions in PyTorch that decouple how you define your data from how you efficiently iterate over it in training loops."
      ],
      "metadata": {
        "id": "mWUcjC_eZwy6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset Class**\n",
        "\n",
        "The Dataset class is essentially a blueprint. When you create a custom Dataset, you decide how data is loaded and returned.\n",
        "\n",
        "It defines:\n",
        "\n",
        "- `__init__()` which tells how data should be loaded.\n",
        "\n",
        "- `__len__()` which returns the total number of samples.This will be used to calculate number of batches based on batch size and lenght of data.\n",
        "\n",
        "- `__getitem__(index)` which returns the data (and label) at the\n",
        "given index.\n",
        "\n",
        "---\n",
        "\n",
        "Dataset class does the loading(reading) of the data and it remembers where data is present in your memory.\n",
        "\n",
        "**Dataset Class is an Abstract Class.**\n",
        "\n",
        "Thus we need to create all the three methods in our custom datasetclass."
      ],
      "metadata": {
        "id": "FlaVvAh9Zwwy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DataLoader Class**\n",
        "\n",
        "The DataLoader wraps a Dataset and handles batching, shuffling, and parallel loading for you.\n",
        "\n",
        "DataLoader Control Flow:\n",
        "\n",
        "- At the start of each epoch, the DataLoader (if shuffle=True)\n",
        "shuffles indices(using a sampler).\n",
        "\n",
        "- It divides the indices into chunks of batch_size.\n",
        "\n",
        "- for each index in the chunk of batch_size, data samples are fetched from the Dataset object using get item because it gives item based on index.\n",
        "\n",
        "- The samples are then collected and combined into a batch (using `collate_fn`).\n",
        "\n",
        "- The batch is returned to the main training loop.\n",
        "\n",
        "---\n",
        "\n",
        "DataLoader Class works on creating and extarcting batches from that loaded data which helps to create mini batch."
      ],
      "metadata": {
        "id": "pcMfE94JZwuQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-6iHPzXtUdvB"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import make_classification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy classification dataset\n",
        "x , y = make_classification(\n",
        "    n_samples=100,\n",
        "    n_features=2,\n",
        "    n_informative=2,\n",
        "    n_redundant=0,\n",
        "    n_classes=2,\n",
        "    random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "dAjS6b9vsYUf"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cg1JrhMs9oo",
        "outputId": "3d85029a-f5a0-4b36-d6ef-e63df042311e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.55942643,  2.38869353],\n",
              "       [ 1.31217492, -0.7173148 ],\n",
              "       [-1.5598485 , -1.92487377],\n",
              "       [-2.2813861 , -0.1368559 ],\n",
              "       [ 1.56070438, -0.42795824],\n",
              "       [-0.80804463,  1.19664076],\n",
              "       [-0.27062383, -2.25553963],\n",
              "       [ 0.480502  ,  0.54914434],\n",
              "       [-1.20757158, -1.26898369],\n",
              "       [ 0.25415746, -1.79532002],\n",
              "       [ 2.59123946,  0.24472415],\n",
              "       [ 0.07123641,  0.49429823],\n",
              "       [-1.17762637, -1.20592943],\n",
              "       [ 0.93343952,  0.68811892],\n",
              "       [ 1.65214494, -0.35885569],\n",
              "       [-1.40735658, -1.56826626],\n",
              "       [ 1.02255619, -1.08324727],\n",
              "       [-0.81680628, -0.6795874 ],\n",
              "       [ 1.50575249, -0.38919817],\n",
              "       [-2.17105282, -0.04862909],\n",
              "       [ 0.71479373, -1.42922002],\n",
              "       [-0.15013844, -0.11708689],\n",
              "       [-1.4117586 , -1.5332749 ],\n",
              "       [-2.58590856, -0.40925706],\n",
              "       [ 0.82600732, -1.05383855],\n",
              "       [-0.07133524,  0.08896214],\n",
              "       [ 0.6273745 , -1.32933233],\n",
              "       [ 1.65882246, -0.43131517],\n",
              "       [ 1.2798899 ,  1.25896077],\n",
              "       [ 0.25058844,  0.13979096],\n",
              "       [-0.05319823,  1.85605469],\n",
              "       [-2.05832072, -2.52343407],\n",
              "       [-2.02632079,  0.06194498],\n",
              "       [-1.65830375, -1.57127256],\n",
              "       [ 1.28008347,  1.28938375],\n",
              "       [ 0.96423311,  0.55600276],\n",
              "       [-1.87653774,  0.23085877],\n",
              "       [-0.94275087,  1.10009583],\n",
              "       [-1.09831681, -0.86023823],\n",
              "       [-0.38566776,  0.01722979],\n",
              "       [ 0.32725188,  0.62453032],\n",
              "       [ 1.03110238,  0.55448398],\n",
              "       [-0.50833095,  1.48052803],\n",
              "       [ 0.68057323,  1.02703224],\n",
              "       [ 1.08266027, -0.98021491],\n",
              "       [-1.40210053, -1.72067112],\n",
              "       [ 0.85239186, -1.14658127],\n",
              "       [ 0.06845616, -1.73597973],\n",
              "       [ 1.53703587, -0.53355799],\n",
              "       [ 0.77861318, -1.20946429],\n",
              "       [-1.64832073,  0.4176729 ],\n",
              "       [ 0.66530077,  0.88629356],\n",
              "       [ 1.17869556,  1.09180466],\n",
              "       [ 1.09885263,  1.25291095],\n",
              "       [-0.91916686,  1.06173727],\n",
              "       [-1.54851014, -1.43534875],\n",
              "       [-0.3313761 ,  1.56931739],\n",
              "       [ 0.4505902 , -1.50077611],\n",
              "       [-1.00634985, -1.14054824],\n",
              "       [-1.13207427,  0.89075877],\n",
              "       [-0.53963044, -0.72427983],\n",
              "       [ 1.08659413, -1.00544254],\n",
              "       [ 0.82584805,  0.53479393],\n",
              "       [ 0.34129395,  0.57304248],\n",
              "       [-1.28568005,  0.73019848],\n",
              "       [ 1.15199146, -0.71352532],\n",
              "       [-1.36045573, -1.64060704],\n",
              "       [-2.3279946 , -0.17593681],\n",
              "       [-0.34898484,  1.56010259],\n",
              "       [ 1.17329352,  0.73644435],\n",
              "       [ 1.57233676,  1.4991983 ],\n",
              "       [-1.69302842, -1.61453273],\n",
              "       [-1.03223274,  0.94800532],\n",
              "       [ 1.83991037,  2.30450019],\n",
              "       [ 1.53313849,  1.74147706],\n",
              "       [-1.43483867, -1.42695838],\n",
              "       [ 1.46830827, -0.48949935],\n",
              "       [ 2.36867367,  2.25661188],\n",
              "       [-1.21722043, -1.36716377],\n",
              "       [-0.78736523,  1.18739045],\n",
              "       [-1.54446032, -1.51042899],\n",
              "       [ 1.98522348, -0.02185231],\n",
              "       [ 0.15513175,  1.99805321],\n",
              "       [-2.00347738, -2.39955005],\n",
              "       [ 2.52983424,  1.94087643],\n",
              "       [ 0.86199147,  1.35970566],\n",
              "       [ 1.35536951,  1.15509316],\n",
              "       [ 1.03967019, -0.92169432],\n",
              "       [-1.82037691,  0.25995672],\n",
              "       [-0.79371387, -0.9311259 ],\n",
              "       [-0.78284083,  1.20852705],\n",
              "       [-2.00918545, -2.24181979],\n",
              "       [ 1.3727107 ,  1.07954187],\n",
              "       [-0.57500215, -0.3751207 ],\n",
              "       [ 1.86378908,  1.59488828],\n",
              "       [ 1.80474148, -0.14994102],\n",
              "       [ 1.15466088, -0.95548162],\n",
              "       [-0.56772453,  1.38991764],\n",
              "       [-1.15806823,  0.86561977],\n",
              "       [-1.75518644,  0.36016958]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L_Ujlfxvs-G5",
        "outputId": "4e8ae16d-32dd-476f-ee1e-f116de02d343"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCuM9Nyns-wj",
        "outputId": "d5696ce2-89d6-4229-dcac-39b339525ee7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oVSHpisCtB3V",
        "outputId": "1b697b1e-351d-43a3-a80e-6f440ac591c0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1,\n",
              "       0, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1,\n",
              "       1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1,\n",
              "       0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1,\n",
              "       0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FniNH_1NtCxN",
        "outputId": "459cbf59-2564-4f10-9d1d-0040d94f0816"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(100,)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dX4n5TLctDaE",
        "outputId": "79fc61f3-2776-418c-b271-07e3272e63d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert them to tensors\n",
        "x_tensor = torch.tensor(x, dtype= torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype= torch.long)"
      ],
      "metadata": {
        "id": "o5djtOPstFMM"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PImKC5q3tcYF",
        "outputId": "6e1c6dfa-ea3b-49cd-8438-231d391b33ae"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.5594,  2.3887],\n",
              "        [ 1.3122, -0.7173],\n",
              "        [-1.5598, -1.9249],\n",
              "        [-2.2814, -0.1369],\n",
              "        [ 1.5607, -0.4280],\n",
              "        [-0.8080,  1.1966],\n",
              "        [-0.2706, -2.2555],\n",
              "        [ 0.4805,  0.5491],\n",
              "        [-1.2076, -1.2690],\n",
              "        [ 0.2542, -1.7953],\n",
              "        [ 2.5912,  0.2447],\n",
              "        [ 0.0712,  0.4943],\n",
              "        [-1.1776, -1.2059],\n",
              "        [ 0.9334,  0.6881],\n",
              "        [ 1.6521, -0.3589],\n",
              "        [-1.4074, -1.5683],\n",
              "        [ 1.0226, -1.0832],\n",
              "        [-0.8168, -0.6796],\n",
              "        [ 1.5058, -0.3892],\n",
              "        [-2.1711, -0.0486],\n",
              "        [ 0.7148, -1.4292],\n",
              "        [-0.1501, -0.1171],\n",
              "        [-1.4118, -1.5333],\n",
              "        [-2.5859, -0.4093],\n",
              "        [ 0.8260, -1.0538],\n",
              "        [-0.0713,  0.0890],\n",
              "        [ 0.6274, -1.3293],\n",
              "        [ 1.6588, -0.4313],\n",
              "        [ 1.2799,  1.2590],\n",
              "        [ 0.2506,  0.1398],\n",
              "        [-0.0532,  1.8561],\n",
              "        [-2.0583, -2.5234],\n",
              "        [-2.0263,  0.0619],\n",
              "        [-1.6583, -1.5713],\n",
              "        [ 1.2801,  1.2894],\n",
              "        [ 0.9642,  0.5560],\n",
              "        [-1.8765,  0.2309],\n",
              "        [-0.9428,  1.1001],\n",
              "        [-1.0983, -0.8602],\n",
              "        [-0.3857,  0.0172],\n",
              "        [ 0.3273,  0.6245],\n",
              "        [ 1.0311,  0.5545],\n",
              "        [-0.5083,  1.4805],\n",
              "        [ 0.6806,  1.0270],\n",
              "        [ 1.0827, -0.9802],\n",
              "        [-1.4021, -1.7207],\n",
              "        [ 0.8524, -1.1466],\n",
              "        [ 0.0685, -1.7360],\n",
              "        [ 1.5370, -0.5336],\n",
              "        [ 0.7786, -1.2095],\n",
              "        [-1.6483,  0.4177],\n",
              "        [ 0.6653,  0.8863],\n",
              "        [ 1.1787,  1.0918],\n",
              "        [ 1.0989,  1.2529],\n",
              "        [-0.9192,  1.0617],\n",
              "        [-1.5485, -1.4353],\n",
              "        [-0.3314,  1.5693],\n",
              "        [ 0.4506, -1.5008],\n",
              "        [-1.0063, -1.1405],\n",
              "        [-1.1321,  0.8908],\n",
              "        [-0.5396, -0.7243],\n",
              "        [ 1.0866, -1.0054],\n",
              "        [ 0.8258,  0.5348],\n",
              "        [ 0.3413,  0.5730],\n",
              "        [-1.2857,  0.7302],\n",
              "        [ 1.1520, -0.7135],\n",
              "        [-1.3605, -1.6406],\n",
              "        [-2.3280, -0.1759],\n",
              "        [-0.3490,  1.5601],\n",
              "        [ 1.1733,  0.7364],\n",
              "        [ 1.5723,  1.4992],\n",
              "        [-1.6930, -1.6145],\n",
              "        [-1.0322,  0.9480],\n",
              "        [ 1.8399,  2.3045],\n",
              "        [ 1.5331,  1.7415],\n",
              "        [-1.4348, -1.4270],\n",
              "        [ 1.4683, -0.4895],\n",
              "        [ 2.3687,  2.2566],\n",
              "        [-1.2172, -1.3672],\n",
              "        [-0.7874,  1.1874],\n",
              "        [-1.5445, -1.5104],\n",
              "        [ 1.9852, -0.0219],\n",
              "        [ 0.1551,  1.9981],\n",
              "        [-2.0035, -2.3995],\n",
              "        [ 2.5298,  1.9409],\n",
              "        [ 0.8620,  1.3597],\n",
              "        [ 1.3554,  1.1551],\n",
              "        [ 1.0397, -0.9217],\n",
              "        [-1.8204,  0.2600],\n",
              "        [-0.7937, -0.9311],\n",
              "        [-0.7828,  1.2085],\n",
              "        [-2.0092, -2.2418],\n",
              "        [ 1.3727,  1.0795],\n",
              "        [-0.5750, -0.3751],\n",
              "        [ 1.8638,  1.5949],\n",
              "        [ 1.8047, -0.1499],\n",
              "        [ 1.1547, -0.9555],\n",
              "        [-0.5677,  1.3899],\n",
              "        [-1.1581,  0.8656],\n",
              "        [-1.7552,  0.3602]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(x_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fa3ciHitnwk",
        "outputId": "878bd979-9f81-4feb-e280-3c8341fa8794"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPLB_DaStpF8",
        "outputId": "8a483f52-e08d-48ca-f631-ec589ffb9bcc"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0,\n",
              "        1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
              "        1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0,\n",
              "        0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1,\n",
              "        1, 0, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(y_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rjy0bpdttqAM",
        "outputId": "f5146d4e-e1b9-4257-f416-889d758f94a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import the from utlility data class**"
      ],
      "metadata": {
        "id": "44QxrqJDtut4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "qXUJ_aDvtryb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a custom dataset class with 3 methods.\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "\n",
        "  # How data is loaded\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "\n",
        "  # Lenght of data\n",
        "  def __len__(self):\n",
        "    # Number of rows\n",
        "    return self.features.shape[0]\n",
        "\n",
        "  # get item at index\n",
        "  def __getitem__(self, index):\n",
        "    # return item at that index\n",
        "    return self.features[index], self.labels[index]"
      ],
      "metadata": {
        "id": "S-FdPfdWt3S1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Object of that Class\n",
        "\n",
        "# Feature and Label\n",
        "dataset = CustomDataset(x_tensor, y_tensor)"
      ],
      "metadata": {
        "id": "wojj_eZOuczt"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjkhCClPulks",
        "outputId": "3751aa00-683a-4b3e-d8b5-94536a3a7fd3"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.CustomDataset at 0x7c344dbea960>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5HzL_TfSum28",
        "outputId": "6eab7455-7a47-46e9-fb81-f742e4e2803a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0bkSyLQhupFt",
        "outputId": "8b7991bd-e7bf-4246-c6f3-1ecd2bb8af39"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-2.2814, -0.1369]), tensor(0))"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Loader Class**"
      ],
      "metadata": {
        "id": "jUvk7OBFuzxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "We dont need to create any data loader class.\n",
        "Just create instance of it and pass dataset, batchsize and shuffle True or False.\n",
        "\"\"\"\n",
        "\n",
        "# dataloader = DataLoader(dataset_object, batch_size, Shuffling)\n",
        "dataloader = DataLoader(dataset, batch_size=4, shuffle=True)"
      ],
      "metadata": {
        "id": "aTa-bT3iurnW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5KLAh3mcvMNV",
        "outputId": "41c340ea-c474-4bef-caee-e25b9aae0f2a"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataloader.DataLoader at 0x7c344cfc0d40>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Extract Data with data loader\n",
        "\"\"\"\n",
        "\n",
        "# Simply run a loop and extract info which is there in data loader, We have features and labels\n",
        "\n",
        "for feature,label in dataloader:\n",
        "  print(feature)\n",
        "  print(label)\n",
        "  print('-'*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36x_H4C9vNLl",
        "outputId": "e4639154-9bf4-4619-b538-5f0a1c99515c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 1.9852, -0.0219],\n",
            "        [-2.3280, -0.1759],\n",
            "        [-0.2706, -2.2555],\n",
            "        [-0.9192,  1.0617]])\n",
            "tensor([1, 0, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 0.8524, -1.1466],\n",
            "        [ 0.4805,  0.5491],\n",
            "        [-1.0063, -1.1405],\n",
            "        [-2.0263,  0.0619]])\n",
            "tensor([1, 1, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.5083,  1.4805],\n",
            "        [-1.5598, -1.9249],\n",
            "        [-1.4118, -1.5333],\n",
            "        [-1.5445, -1.5104]])\n",
            "tensor([0, 0, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[2.5912, 0.2447],\n",
            "        [0.8620, 1.3597],\n",
            "        [1.5331, 1.7415],\n",
            "        [0.2506, 0.1398]])\n",
            "tensor([1, 1, 1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.1581,  0.8656],\n",
            "        [ 1.1733,  0.7364],\n",
            "        [-0.5750, -0.3751],\n",
            "        [ 1.6521, -0.3589]])\n",
            "tensor([0, 1, 0, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 0.9642,  0.5560],\n",
            "        [ 1.8399,  2.3045],\n",
            "        [-1.2172, -1.3672],\n",
            "        [ 0.3273,  0.6245]])\n",
            "tensor([1, 1, 0, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.0311,  0.5545],\n",
            "        [-1.8204,  0.2600],\n",
            "        [ 0.7786, -1.2095],\n",
            "        [ 1.5607, -0.4280]])\n",
            "tensor([1, 0, 1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[-2.5859, -0.4093],\n",
            "        [-0.8080,  1.1966],\n",
            "        [ 1.0397, -0.9217],\n",
            "        [-2.0092, -2.2418]])\n",
            "tensor([0, 0, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.3314,  1.5693],\n",
            "        [-0.3490,  1.5601],\n",
            "        [ 1.4683, -0.4895],\n",
            "        [ 0.5594,  2.3887]])\n",
            "tensor([0, 0, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.6588, -0.4313],\n",
            "        [ 0.3413,  0.5730],\n",
            "        [-0.3857,  0.0172],\n",
            "        [-1.4348, -1.4270]])\n",
            "tensor([1, 1, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.6483,  0.4177],\n",
            "        [ 1.0866, -1.0054],\n",
            "        [ 1.0827, -0.9802],\n",
            "        [-1.6930, -1.6145]])\n",
            "tensor([0, 1, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.8047, -0.1499],\n",
            "        [ 0.7148, -1.4292],\n",
            "        [ 0.4506, -1.5008],\n",
            "        [ 0.8258,  0.5348]])\n",
            "tensor([1, 1, 1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.3122, -0.7173],\n",
            "        [-0.0532,  1.8561],\n",
            "        [ 0.9334,  0.6881],\n",
            "        [ 1.1787,  1.0918]])\n",
            "tensor([1, 0, 1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.0983, -0.8602],\n",
            "        [ 0.6653,  0.8863],\n",
            "        [ 0.6806,  1.0270],\n",
            "        [-0.5396, -0.7243]])\n",
            "tensor([0, 1, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.5723,  1.4992],\n",
            "        [-2.1711, -0.0486],\n",
            "        [-1.4074, -1.5683],\n",
            "        [-1.1776, -1.2059]])\n",
            "tensor([1, 0, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.7828,  1.2085],\n",
            "        [-1.0322,  0.9480],\n",
            "        [-0.8168, -0.6796],\n",
            "        [-1.1321,  0.8908]])\n",
            "tensor([0, 0, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.0226, -1.0832],\n",
            "        [ 2.3687,  2.2566],\n",
            "        [ 1.3554,  1.1551],\n",
            "        [ 0.1551,  1.9981]])\n",
            "tensor([1, 1, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 0.2542, -1.7953],\n",
            "        [ 1.0989,  1.2529],\n",
            "        [-0.7937, -0.9311],\n",
            "        [ 1.8638,  1.5949]])\n",
            "tensor([1, 1, 0, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.3605, -1.6406],\n",
            "        [ 1.3727,  1.0795],\n",
            "        [-1.4021, -1.7207],\n",
            "        [-0.5677,  1.3899]])\n",
            "tensor([0, 1, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-2.0583, -2.5234],\n",
            "        [-2.2814, -0.1369],\n",
            "        [ 0.6274, -1.3293],\n",
            "        [-1.6583, -1.5713]])\n",
            "tensor([0, 0, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.2076, -1.2690],\n",
            "        [-0.0713,  0.0890],\n",
            "        [-0.7874,  1.1874],\n",
            "        [-1.8765,  0.2309]])\n",
            "tensor([0, 0, 0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-0.1501, -0.1171],\n",
            "        [ 0.0712,  0.4943],\n",
            "        [ 1.1520, -0.7135],\n",
            "        [-1.5485, -1.4353]])\n",
            "tensor([1, 0, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.1547, -0.9555],\n",
            "        [-1.2857,  0.7302],\n",
            "        [ 1.2799,  1.2590],\n",
            "        [-0.9428,  1.1001]])\n",
            "tensor([1, 0, 1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.7552,  0.3602],\n",
            "        [ 1.5370, -0.5336],\n",
            "        [ 2.5298,  1.9409],\n",
            "        [ 0.0685, -1.7360]])\n",
            "tensor([0, 1, 1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[ 0.8260, -1.0538],\n",
            "        [-2.0035, -2.3995],\n",
            "        [ 1.2801,  1.2894],\n",
            "        [ 1.5058, -0.3892]])\n",
            "tensor([1, 0, 1, 1])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Where to Apply Transformation in this**"
      ],
      "metadata": {
        "id": "JSq53NZnv2EF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We do this in DataCLass `get` method.\n",
        "Transformation before returning the row\n",
        "\n",
        "What kind of transformation\n",
        "\n",
        "Example\n",
        "\n",
        "1. For Images:\n",
        "\n",
        "- resize\n",
        "- b/w to rgb and vice versa\n",
        "- data augmentation\n",
        "- etc etc...\n",
        "\n",
        "2. For Text:\n",
        "\n",
        "- lower case\n",
        "- lemmatization\n",
        "- stop word removal\n",
        "- etc etc..."
      ],
      "metadata": {
        "id": "VAqW1wiov7YS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Parallelization**\n",
        "\n",
        "`n_workers`"
      ],
      "metadata": {
        "id": "MN4XGVjx2E68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imagine the entire data loading and training process for one epoch with num_workers=4:\n",
        "\n",
        "**Assumptions:**\n",
        "- Total samples: 10,000\n",
        "- Batch size: 32\n",
        "- Workers (num_workers): 4\n",
        "- Approximately 312 full batches per epoch (10000 / 32 ≈ 312)."
      ],
      "metadata": {
        "id": "jsMg6rPs2Lnq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Workflow"
      ],
      "metadata": {
        "id": "PggSvQaJ2TTG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.Sampler and Batch Creation (Main Process):**\n",
        "\n",
        "Before training starts for the epoch, the DataLoader’s sampler generates a shuffled list of all 10,000 indices. These\n",
        "are then grouped into 312 batches of 32 indices each. All these batches are queued up, ready to be fetched by\n",
        "workers."
      ],
      "metadata": {
        "id": "mZS-9BsS2Zr5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2.Parallel Data Loading (Workers):**\n",
        "\n",
        "At the start of the training epoch, you run a training loop like:\n",
        "```py\n",
        "for batch_data, batch_labels in dataloader:\n",
        "  # Training logic\n",
        "```\n",
        "\n",
        "\n",
        "Under the hood, as soon as you start iterating over dataloader, it dispatches the first four batches of indices\n",
        "to the four workers:\n",
        "\n",
        "- Worker #1 loads batch 1 (indices [batch_1_indices])\n",
        "- Worker #2 loads batch 2 (indices [batch_2_indices])\n",
        "- Worker #3 loads batch 3 (indices [batch_3_indices])\n",
        "- Worker #4 loads batch 4 (indices [batch_4_indices])\n",
        "\n",
        "*Each worker:*\n",
        "\n",
        "- Fetches the corresponding samples by calling __getitem__ on the dataset for each index in that batch.\n",
        "\n",
        "- Applies any defined transforms and passes the samples through collate_fn to form a single batch tensor."
      ],
      "metadata": {
        "id": "vMlMOi_c2jjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3.First Batch Returned to Main Process:**\n",
        "\n",
        "Whichever worker finishes first sends its fully prepared batch (e.g., batch 1) back to the main process.\n",
        "\n",
        "As soon as the main process gets this first prepared batch, it yields it to your training loop, so your code for\n",
        "batch_data, batch_labels in dataloader:receives (batch_data, batch_labels) for the first batch.\n"
      ],
      "metadata": {
        "id": "VBZMRpV82jb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4.Model Training on the Main Process:**\n",
        "\n",
        "While you are now performing the forward pass, computing loss, and doing backpropagation on the first\n",
        "batch, the other three workers are still preparing their batches in parallel.\n",
        "\n",
        "By the time you finish updating your model parameters for the first batch, the DataLoader likely has the\n",
        "second, third, or even more batches ready to go (depending on processing speed and hardware).\n"
      ],
      "metadata": {
        "id": "do62c4Hg2jYT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5.Continuous Processing:**\n",
        "\n",
        "As soon as a worker finishes its batch, it grabs the next batch of indices from the queue.\n",
        "\n",
        "For example: after Worker #1 finishes with batch 1, it immediately starts on batch 5. After Worker #2\n",
        "finishes batch 2, it takes batch 6, and so forth.\n",
        "\n",
        "This creates a pipeline effect: at any given moment, up to 4 batches are being prepared concurrently."
      ],
      "metadata": {
        "id": "Z8M8zBuq2jUn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6.Loop Progression:**\n",
        "\n",
        "Your training loop simply sees:\n",
        "```python\n",
        "for batch_data, batch_labels in dataloader:\n",
        "    # forward pass\n",
        "    # loss computation\n",
        "    # backward pass\n",
        "    # optimizer step\n",
        "```\n",
        "\n",
        "Each iteration, it gets a new, ready-to-use batch without long I/O waits, because the workers have been pre-\n",
        "loading and processing data in parallel."
      ],
      "metadata": {
        "id": "Dqprldye2jP1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**7. End of the Epoch:**\n",
        "\n",
        "After ~312 iterations, all batches have been processed. All indices have been consumed, so the DataLoader\n",
        "has no more batches to yield.\n",
        "\n",
        "The epoch ends. If shuffle=True, on the next epoch, the sampler reshuffles indices, and the whole process\n",
        "repeats with workers again loading data in parallel.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "zZ2Faz543thz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **How does shuffling Happen in Data Loader**\n",
        "\n",
        "This uses sampler"
      ],
      "metadata": {
        "id": "ygNMIQ-v4oJh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In PyTorch, the sampler in the DataLoader determines the strategy for selecting samples from the dataset during data loading. It controls how indices of the dataset are drawn for each batch."
      ],
      "metadata": {
        "id": "ezqRTBlE4e8s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Types of Samplers**\n",
        "\n",
        "PyTorch provides several predefined samplers, and you can create custom ones:\n",
        "1. **SequentialSampler**:\n",
        "\n",
        "- Samples elements sequentially, in the order they appear in the dataset.\n",
        "\n",
        "- Default when shuffle=False.\n",
        "\n",
        "- Should be used when working with timeseries data\n",
        "\n",
        "2. **RandomSampler**:\n",
        "\n",
        "- Samples elements randomly without replacement.\n",
        "\n",
        "- Default when shuffle=True."
      ],
      "metadata": {
        "id": "wttZAXes4e28"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also create custom Sampling Strategies.\n",
        "\n",
        "Suppose we need sampling but in that sample we also need it to follow a particular types of Distribution."
      ],
      "metadata": {
        "id": "uP5-y2Rf4ezB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Collate Function**\n",
        "\n",
        "The collate_fn in PyTorch's DataLoader is a function that specifies how to combine a list of samples from a dataset into a single batch.\n",
        "\n",
        "By default, the DataLoader uses a simple batch collation mechanism, but collate_fn allows you to customize how the data should be processed and batched."
      ],
      "metadata": {
        "id": "8yC6hYzU530v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Why do we need custom merging strategy for merging data to create batches**\n",
        "\n",
        "- When there will be difference size of tensor in our rows and while merging they both cannot be merged because the shape is not same"
      ],
      "metadata": {
        "id": "97eXTy0j4eqh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loader Important Parameters**\n",
        "\n",
        "The DataLoader class in PyTorch comes with several parameters that allow you to customize how data is loaded, batched, and preprocessed. Some of the most commonly used and important parameters include:"
      ],
      "metadata": {
        "id": "AsTtaYa46mpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. dataset(mandatory) :\n",
        "The Dataset from which the DataLoader will pull data.\n",
        "Must be a subclass of torch.utils.data.Dataset that implements __getitem__ and\n",
        "__len__.\n"
      ],
      "metadata": {
        "id": "wVwFos5M6woP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. batch_size: How many samples per batch to load. Default is 1. Larger batch sizes can speed up training on GPUs but require more memory."
      ],
      "metadata": {
        "id": "XmZnxSZy67ma"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. shuffle: If True, the DataLoader will shuffle the dataset indices each epoch.\n",
        "Helpful to avoid the model becoming too dependent on the order of samples."
      ],
      "metadata": {
        "id": "wkITiz0167i4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. num_workers:\n",
        "The number of worker processes used to load data in parallel.\n",
        "Setting num_workers > 0 can speed up data loading by leveraging multiple CPU\n",
        "cores, especially if I/O or preprocessing is a bottleneck."
      ],
      "metadata": {
        "id": "-o55TwBI67eX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. pin_memory:\n",
        "If True, the DataLoader will copy tensors into pinned (page-locked) memory before\n",
        "returning them.\n",
        "This can improve GPU transfer speed and thus overall training throughput,\n",
        "particularly on CUDA systems.\n"
      ],
      "metadata": {
        "id": "FrTTkE4567Zc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. drop_last:\n",
        "If True, the DataLoader will drop the last incomplete batch if the total number of samples is not divisible by the batch size.\n",
        "Useful when exact batch sizes are required (for example, in some batch\n",
        "normalization scenarios)."
      ],
      "metadata": {
        "id": "8owLpLew7ZKF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. collate_fn:\n",
        "A callable that processes a list of samples into a batch (the default simply stacks tensors).\n",
        "Custom collate_fn can handle variable-length sequences, perform custom batching logic, or handle complex data structures."
      ],
      "metadata": {
        "id": "JaUnvYtC7ZH_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. sampler:\n",
        "sampler defines the strategy for drawing samples (e.g., for handling imbalanced\n",
        "classes, or custom sampling strategies).\n",
        "batch_sampler works at the batch level, controlling how batches are formed.\n",
        "Typically, you don’t need to specify these if you are using batch_size and shuffle.\n",
        "However, they provide lower-level control if you have advanced requirements.\n"
      ],
      "metadata": {
        "id": "ZksvnWgN7ZFR"
      }
    }
  ]
}